# Maintainer: Alexander F. RÃ¸dseth <xyproto@archlinux.org>
# Contributor: Matt Harrison <matt@harrison.us.com>

pkgname=ollama-rocm
pkgdesc='Create, run and share large language models (LLMs) with ROCm'
pkgver=0.1.32
pkgrel=1
arch=(x86_64)
url='https://github.com/ollama/ollama'
license=(MIT)
_ollamacommit=fb9580df85c562295d919b6c2632117d3d8cea89 # tag: v0.1.32
# The llama.cpp git submodule commit hash can be found here:
# https://github.com/ollama/ollama/tree/v0.1.32/llm
_llama_cpp_commit=7593639ce335e8d7f89aa9a54d616951f273af60
makedepends=(clblast cmake git go rocm-hip-sdk rocm-opencl-sdk)
provides=(ollama)
conflicts=(ollama)
source=(git+$url#tag=v$pkgver
        llama.cpp::git+https://github.com/ggerganov/llama.cpp#commit=$_llama_cpp_commit
        ollama.service
        sysusers.conf
        tmpfiles.d
        "mixtral.patch::https://github.com/ollama/ollama/commit/435cc866a3fbabb5029b8a2496631847a871616f.diff")
b2sums=('12b952df20a194e7b41040574b449aa9781dc40e9316d6d459caf0228c51c588d93a79223fe4009ca6532dffb9846853a6e74769db15c25135032bf5ccfde65e'
        'de8f50fb5a99a0251f6a0f3d596975cf43ad76770ac2edeaac6497b81214a7f31574b499f10b77f0542f9d46c5d33e0f5ac1e49fa72343d860c70fd3ca0ca113'
        '1397e36c703799e60bc53ec365f1f9f8152091d48d704adea2fda3e7599d938274bec825b09b0a7eb56b7fce2bed1b6d1a034687138d68c51a0133d75a1eb12f'
        '3aabf135c4f18e1ad745ae8800db782b25b15305dfeaaa031b4501408ab7e7d01f66e8ebb5be59fc813cfbff6788d08d2e48dcf24ecc480a40ec9db8dbce9fec'
        'e8f2b19e2474f30a4f984b45787950012668bf0acb5ad1ebb25cd9776925ab4a6aa927f8131ed53e35b1c71b32c504c700fe5b5145ecd25c7a8284373bb951ed'
        '3326071dcd80ae8078d002cb98f7f61cb2493cc0de33cf2923f0e28675ebb115455c82be7cb0890501accc0fd34b1805ac0ae230dcac970570331254aaf756c0')

prepare() {
  cd ${pkgname/-rocm}

  patch -p1 -i ../mixtral.patch

  rm -frv llm/llama.cpp

  # Copy git submodule files instead of symlinking because the build process is sensitive to symlinks.
  cp -r "$srcdir/llama.cpp" llm/llama.cpp

  # Turn LTO on and set the build type to Release
  sed -i 's,T_CODE=on,T_CODE=on -D LLAMA_LTO=on -D LLAMA_HIPBLAS=1 -D AMDGPU_TARGETS=gfx1030 -D CMAKE_BUILD_TYPE=Release,g' llm/generate/gen_linux.sh

  sed -i 's,g++,/opt/rocm/llvm/bin/clang++,g' llm/generate/gen_common.sh
}

build() {
  cd ${pkgname/-rocm}

  export CC=/opt/rocm/llvm/bin/clang
  export CXX=/opt/rocm/llvm/bin/clang++
  export CFLAGS="$CFLAGS -fcf-protection=none"
  export CXXFLAGS="$CXXFLAGS -fcf-protection=none"
  export LLAMA_CCACHE=OFF
  export OLLAMA_CUSTOM_CPU_DEFS="-DLLAMA_AVX=on -DLLAMA_AVX2=on -DAMDGPU_TARGETS=gfx1030 -DLLAMA_F16C=on -DLLAMA_FMA=on -DLLAMA_LTO=on -DLLAMA_HIPBLAS=1 -DCMAKE_BUILD_TYPE=Release"
  export CGO_CFLAGS="$CFLAGS" CGO_CPPFLAGS="$CPPFLAGS" CGO_CXXFLAGS="$CXXFLAGS" CGO_LDFLAGS="$LDFLAGS"

  go generate ./...
  go build -buildmode=pie -trimpath -mod=readonly -modcacherw -ldflags=-linkmode=external \
    -ldflags=-buildid='' -ldflags="-X=github.com/ollama/ollama/version.Version=$pkgver" -tags rocm
}

check() {
  cd ${pkgname/-rocm}
  ./ollama --version > /dev/null
  go test .
}

package() {
  install -Dm755 ${pkgname/-rocm}/${pkgname/-rocm} "$pkgdir/usr/bin/${pkgname/-rocm}"
  install -dm755 "$pkgdir/var/lib/ollama"
  install -Dm644 ollama.service "$pkgdir/usr/lib/systemd/system/ollama.service"
  install -Dm644 sysusers.conf "$pkgdir/usr/lib/sysusers.d/ollama.conf"
  install -Dm644 tmpfiles.d "$pkgdir/usr/lib/tmpfiles.d/ollama.conf"
  install -Dm644 ${pkgname/-rocm}/LICENSE "$pkgdir/usr/share/licenses/$pkgname/LICENSE"
}
