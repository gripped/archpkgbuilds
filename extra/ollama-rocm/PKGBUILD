# Maintainer: Alexander F. RÃ¸dseth <xyproto@archlinux.org>
# Contributor: Steven Allen <steven@stebalien.com>
# Contributor: Matt Harrison <matt@harrison.us.com>
# Contributor: Kainoa Kanter <kainoa@t1c.dev>
# Contributor: Wilken Gottwalt <wilken.gottwalt@posteo.net>

pkgname=ollama-rocm
pkgver=0.3.12
pkgrel=6
pkgdesc='Create, run and share large language models (LLMs) with ROCm'
arch=(x86_64)
url='https://github.com/ollama/ollama'
_llama_cpp_commit=$(curl -sL "https://github.com/ollama/ollama/tree/v$pkgver/llm" | tr ' ' '\n' | tr '"' '\n' | grep ggerganov | cut -d/ -f5 | head -1)
license=(MIT)
provides=(ollama)
conflicts=(ollama)
depends=(hipblas)
optdepends=('rocm-smi-lib: monitor GPU usage with rocm-smi')
makedepends=(clblast cmake git go rocm-hip-sdk rocm-opencl-sdk)
source=(git+$url#tag=v$pkgver
        llama.cpp::git+https://github.com/ggerganov/llama.cpp#commit=$_llama_cpp_commit
        ollama.service
        sysusers.conf
        tmpfiles.d)
b2sums=('7b230922a233f0be967abb40b300125fb59ed64850a109db185954c549e12e5de0c1be094c1cdb67484c9e8705804a3761bfefa517fac1adfb05bcae8d2c197d'
        'e568ac334cf07b69f98c4581f212f2e30bdebf1f285a37e4bb5c8ac31733df9a3c125e6d16b7f6bbe412cedb11d249a07ca9793a487e0ad34810cbb35cd32ee2'
        '031e0809a7f564de87017401c83956d43ac29bd0e988b250585af728b952a27d139b3cad0ab1e43750e2cd3b617287d3b81efc4a70ddd61709127f68bd15eabd'
        '3aabf135c4f18e1ad745ae8800db782b25b15305dfeaaa031b4501408ab7e7d01f66e8ebb5be59fc813cfbff6788d08d2e48dcf24ecc480a40ec9db8dbce9fec'
        'e8f2b19e2474f30a4f984b45787950012668bf0acb5ad1ebb25cd9776925ab4a6aa927f8131ed53e35b1c71b32c504c700fe5b5145ecd25c7a8284373bb951ed')

prepare() {
  echo "Using llama.cpp git submodule commit $_llama_cpp_commit"

  # Prepare the git submodule by copying in files (the build process is sensitive to symlinks)
  rm -frv ollama/llm/llama.cpp
  cp -r llama.cpp ollama/llm/llama.cpp

  # Set the CMake build type to "Release"
  sed -i 's,T_CODE=on,T_CODE=on -D CMAKE_BUILD_TYPE=Release,g' ollama/llm/generate/gen_linux.sh

  # Prepare the ollama-rocm directory for building for ROCm
  cd ollama/llm/generate
  sed -i 's,g++,/opt/rocm/llvm/bin/clang++,g' gen_common.sh
  sed -i 's,T_CODE=on,T_CODE=on -D LLAMA_HIPBLAS=1 -D AMDGPU_TARGETS=gfx1030,g' gen_linux.sh
}

build() {
  export CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++
  export CFLAGS+=' -fcf-protection=none -w' CXXFLAGS+=' -fcf-protection=none -w'
  export LDFLAGS+=' -L/opt/cuda/targets/x86_64-linux/lib/stubs/'
  export CMAKE_DEFS='-DBUILD_SHARED_LIBS=off -DCMAKE_BUILD_TYPE=MinSizeRel'
  export AMDGPU_TARGETS="gfx1030,gfx1100"
  export OLLAMA_CPU_TARGET="cpu_avx2"
  export OLLAMA_CUSTOM_CPU_DEFS='-DGGML_LTO=on'
  export OLLAMA_SKIP_CUDA_GENERATE=on
  export OLLAMA_SKIP_ONEAPI_GENERATE=on
  export ROCM_PATH=/opt/rocm
  export CUDA_LIB_DIR=/disabled

  export CGO_CFLAGS="$CFLAGS" CGO_CPPFLAGS="$CPPFLAGS" CGO_CXXFLAGS="$CXXFLAGS" CGO_LDFLAGS="$LDFLAGS"
  local goflags='-buildmode=pie -trimpath -mod=readonly -modcacherw'
  local ldflags="-linkmode=external -buildid= -X github.com/ollama/ollama/version.Version=$pkgver -X github.com/ollama/ollama/server.mode=release"

  cd ollama
  go generate ./...
  go build $goflags -ldflags="$ldflags" -tags rocm
}

check() {
  ollama/ollama --version > /dev/null
  cd ollama
  go test .
}

package() {
  install -Dm755 ollama/ollama "$pkgdir/usr/bin/ollama"
  install -dm755 "$pkgdir/var/lib/ollama"
  install -Dm644 ollama.service "$pkgdir/usr/lib/systemd/system/ollama.service"
  install -Dm644 sysusers.conf "$pkgdir/usr/lib/sysusers.d/ollama.conf"
  install -Dm644 tmpfiles.d "$pkgdir/usr/lib/tmpfiles.d/ollama.conf"
  install -Dm644 ollama/LICENSE "$pkgdir/usr/share/licenses/$pkgname/LICENSE"
  install -Dm644 ollama/llm/llama.cpp/LICENSE "$pkgdir/usr/share/licenses/$pkgname/llama.cpp-LICENSE"

  ln -s /var/lib/ollama "$pkgdir/usr/share/ollama"
}
